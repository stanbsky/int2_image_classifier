{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from inspect import signature\n",
    "import copy\n",
    "from collections import namedtuple, defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import singledispatch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from itertools import count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Timer():\n",
    "    def __init__(self, synch=None):\n",
    "        self.synch = synch or (lambda: None)\n",
    "        self.synch()\n",
    "        self.times = [time.perf_counter()]\n",
    "        self.total_time = 0.0\n",
    "\n",
    "    def __call__(self, include_in_total=True):\n",
    "        self.synch()\n",
    "        self.times.append(time.perf_counter())\n",
    "        delta_t = self.times[-1] - self.times[-2]\n",
    "        if include_in_total:\n",
    "            self.total_time += delta_t\n",
    "        return delta_t\n",
    "    \n",
    "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "default_table_formats = {float: '{:{w}.4f}', str: '{:>{w}s}', 'default': '{:{w}}', 'title': '{:>{w}s}'}\n",
    "\n",
    "def table_formatter(val, is_title=False, col_width=12, formats=None):\n",
    "    formats = formats or default_table_formats\n",
    "    type_ = lambda val: float if isinstance(val, (float, float)) else type(val)\n",
    "    return (formats['title'] if is_title else formats.get(type_(val), formats['default'])).format(val, w=col_width)\n",
    "\n",
    "def every(n, col): \n",
    "    return lambda data: data[col] % n == 0\n",
    "\n",
    "class Table():\n",
    "    def __init__(self, keys=None, report=(lambda data: True), formatter=table_formatter):\n",
    "        self.keys, self.report, self.formatter = keys, report, formatter\n",
    "        self.log = []\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.log.append(data)\n",
    "        data = {' '.join(p): v for p,v in path_iter(data)}\n",
    "        self.keys = self.keys or data.keys()\n",
    "        if len(self.log) == 1:\n",
    "            print(*(self.formatter(k, True) for k in self.keys))\n",
    "        if self.report(data):\n",
    "            print(*(self.formatter(data[k]) for k in self.keys))\n",
    "            \n",
    "    def df(self):\n",
    "        return pd.DataFrame([{'_'.join(p): v for p,v in path_iter(row)} for row in self.log])    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def preprocess(dataset, transforms):\n",
    "    dataset = copy.copy(dataset) #shallow copy\n",
    "    for transform in transforms:\n",
    "        dataset['data'] = transform(dataset['data'])\n",
    "    return dataset\n",
    "\n",
    "@singledispatch\n",
    "def normalise(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "@normalise.register(np.ndarray) \n",
    "def _(x, mean, std): \n",
    "    #faster inplace for numpy arrays\n",
    "    x = np.array(x, np.float32)\n",
    "    x -= mean\n",
    "    x *= 1.0/std\n",
    "    return x\n",
    "\n",
    "unnormalise = lambda x, mean, std: x*std + mean\n",
    "\n",
    "@singledispatch\n",
    "def pad(x, border):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@pad.register(np.ndarray)\n",
    "def _(x, border): \n",
    "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
    "\n",
    "@singledispatch\n",
    "def transpose(x, source, target):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@transpose.register(np.ndarray)\n",
    "def _(x, source, target):\n",
    "    return x.transpose([source.index(d) for d in target]) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data augmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
    "    def __call__(self, x, x0, y0):\n",
    "        return x[..., y0:y0+self.h, x0:x0+self.w]\n",
    "\n",
    "    def options(self, shape):\n",
    "        *_, H, W = shape\n",
    "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]\n",
    "    \n",
    "    def output_shape(self, shape):\n",
    "        *_, H, W = shape\n",
    "        return (*_, self.h, self.w)\n",
    "\n",
    "@singledispatch\n",
    "def flip_lr(x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@flip_lr.register(np.ndarray)\n",
    "def _(x): \n",
    "    return x[..., ::-1].copy()\n",
    "\n",
    "class FlipLR(namedtuple('FlipLR', ())):\n",
    "    def __call__(self, x, choice):\n",
    "        return flip_lr(x) if choice else x \n",
    "        \n",
    "    def options(self, shape):\n",
    "        return [{'choice': b} for b in [True, False]]\n",
    "\n",
    "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
    "    def __call__(self, x, x0, y0):\n",
    "        x[..., y0:y0+self.h, x0:x0+self.w] = 0.0\n",
    "        return x\n",
    "\n",
    "    def options(self, shape):\n",
    "        *_, H, W = shape\n",
    "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]    \n",
    "    \n",
    "\n",
    "class Transform():\n",
    "    def __init__(self, dataset, transforms):\n",
    "        self.dataset, self.transforms = dataset, transforms\n",
    "        self.choices = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "           \n",
    "    def __getitem__(self, index):\n",
    "        data, labels = self.dataset[index]\n",
    "        data = data.copy()\n",
    "        for choices, f in zip(self.choices, self.transforms):\n",
    "            data = f(data, **choices[index])\n",
    "        return data, labels\n",
    "    \n",
    "    def set_random_choices(self):\n",
    "        self.choices = []\n",
    "        x_shape = self.dataset[0][0].shape\n",
    "        N = len(self)\n",
    "        for t in self.transforms:\n",
    "            self.choices.append(np.random.choice(t.options(x_shape), N))\n",
    "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dict utils"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
    "\n",
    "def path_iter(nested_dict, pfx=()):\n",
    "    for name, val in nested_dict.items():\n",
    "        if isinstance(val, dict): yield from path_iter(val, (*pfx, name))\n",
    "        else: yield ((*pfx, name), val)  \n",
    "\n",
    "def map_nested(func, nested_dict):\n",
    "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k,v in nested_dict.items()}\n",
    "\n",
    "def group_by_key(items):\n",
    "    res = defaultdict(list)\n",
    "    for k, v in items: \n",
    "        res[k].append(v) \n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph building"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sep = '/'\n",
    "\n",
    "def split(path):\n",
    "    i = path.rfind(sep) + 1\n",
    "    return path[:i].rstrip(sep), path[i:]\n",
    "\n",
    "def normpath(path):\n",
    "    #simplified os.path.normpath\n",
    "    parts = []\n",
    "    for p in path.split(sep):\n",
    "        if p == '..': parts.pop()\n",
    "        elif p.startswith(sep): parts = [p]\n",
    "        else: parts.append(p)\n",
    "    return sep.join(parts)\n",
    "\n",
    "has_inputs = lambda node: type(node) is tuple\n",
    "\n",
    "def pipeline(net):\n",
    "    return [(sep.join(path), (node if has_inputs(node) else (node, [-1]))) for (path, node) in path_iter(net)]\n",
    "\n",
    "def build_graph(net):\n",
    "    flattened = pipeline(net)\n",
    "    resolve_input = lambda rel_path, path, idx: normpath(sep.join((path, '..', rel_path))) if isinstance(rel_path, str) else flattened[idx+rel_path][0]\n",
    "    return {path: (node[0], [resolve_input(rel_path, path, idx) for rel_path in node[1]]) for idx, (path, node) in enumerate(flattened)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training utils"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "@singledispatch\n",
    "def cat(*xs):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "@singledispatch\n",
    "def to_numpy(x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
    "    def __call__(self, t):\n",
    "        return np.interp([t], self.knots, self.vals)[0]\n",
    " \n",
    "class Const(namedtuple('Const', ['val'])):\n",
    "    def __call__(self, x):\n",
    "        return self.val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "@cat.register(torch.Tensor)\n",
    "def _(*xs):\n",
    "    return torch.cat(xs)\n",
    "\n",
    "@to_numpy.register(torch.Tensor)\n",
    "def _(x):\n",
    "    return x.detach().cpu().numpy()  \n",
    "\n",
    "@pad.register(torch.Tensor)\n",
    "def _(x, border):\n",
    "    return nn.ReflectionPad2d(border)(x)\n",
    "\n",
    "@transpose.register(torch.Tensor)\n",
    "def _(x, source, target):\n",
    "    return x.permute([source.index(d) for d in target]) \n",
    "\n",
    "def to(*args, **kwargs): \n",
    "    return lambda x: x.to(*args, **kwargs)\n",
    "\n",
    "@flip_lr.register(torch.Tensor)\n",
    "def _(x):\n",
    "    return torch.flip(x, [-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from functools import lru_cache as cache\n",
    "\n",
    "@cache(None)\n",
    "def cifar10(root='./data'):\n",
    "    try:\n",
    "        \n",
    "        download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
    "        return {k: {'data': v.data, 'targets': v.targets} for k,v in [('train', download(train=True)), ('valid', download(train=False))]}\n",
    "    except ImportError:\n",
    "        from tensorflow.keras import datasets\n",
    "        (train_images, train_labels), (valid_images, valid_labels) = datasets.cifar10.load_data()\n",
    "        return {\n",
    "            'train': {'data': train_images, 'targets': train_labels.squeeze()},\n",
    "            'valid': {'data': valid_images, 'targets': valid_labels.squeeze()}\n",
    "        }\n",
    "             \n",
    "cifar10_mean, cifar10_std = [\n",
    "    (125.31, 122.95, 113.87), # equals np.mean(cifar10()['train']['data'], axis=(0,1,2)) \n",
    "    (62.99, 62.09, 66.70), # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
    "]\n",
    "cifar10_classes= 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.set_random_choices = set_random_choices\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
    "        )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.set_random_choices:\n",
    "            self.dataset.set_random_choices() \n",
    "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.dataloader)\n",
    "    \n",
    "#GPU dataloading\n",
    "chunks = lambda data, splits: (data[start:end] for (start, end) in zip(splits, splits[1:]))\n",
    "\n",
    "even_splits = lambda N, num_chunks: np.cumsum([0] + [(N//num_chunks)+1]*(N % num_chunks)  + [N//num_chunks]*(num_chunks - (N % num_chunks)))\n",
    "\n",
    "def shuffled(xs, inplace=False):\n",
    "    xs = xs if inplace else copy.copy(xs) \n",
    "    np.random.shuffle(xs)\n",
    "    return xs\n",
    "\n",
    "def transformed(data, targets, transform, max_options=None, unshuffle=False):\n",
    "    i = torch.randperm(len(data), device=device)\n",
    "    data = data[i]\n",
    "    options = shuffled(transform.options(data.shape), inplace=True)[:max_options]\n",
    "    data = torch.cat([transform(x, **choice) for choice, x in zip(options, chunks(data, even_splits(len(data), len(options))))])\n",
    "    return (data[torch.argsort(i)], targets) if unshuffle else (data, targets[i])\n",
    "\n",
    "class GPUBatches():\n",
    "    def __init__(self, batch_size, transforms=(), dataset=None, shuffle=True, drop_last=False, max_options=None):\n",
    "        self.dataset, self.transforms, self.shuffle, self.max_options = dataset, transforms, shuffle, max_options\n",
    "        N = len(dataset['data'])\n",
    "        self.splits = list(range(0, N+1, batch_size))\n",
    "        if not drop_last and self.splits[-1] != N:\n",
    "            self.splits.append(N)\n",
    "     \n",
    "    def __iter__(self):\n",
    "        data, targets = self.dataset['data'], self.dataset['targets']\n",
    "        for transform in self.transforms:\n",
    "            data, targets = transformed(data, targets, transform, max_options=self.max_options, unshuffle=not self.shuffle)\n",
    "        if self.shuffle:\n",
    "            i = torch.randperm(len(data), device=device)\n",
    "            data, targets = data[i], targets[i]\n",
    "        return ({'input': x.clone(), 'target': y} for (x, y) in zip(chunks(data, self.splits), chunks(targets, self.splits)))\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.splits) - 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Network\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.graph = build_graph(net)\n",
    "        for path, (val, _) in self.graph.items(): \n",
    "            setattr(self, path.replace('/', '_'), val)\n",
    "    \n",
    "    def nodes(self):\n",
    "        return (node for node, _ in self.graph.values())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = dict(inputs)\n",
    "        for k, (node, ins) in self.graph.items():\n",
    "            #only compute nodes that are not supplied as inputs.\n",
    "            if k not in outputs: \n",
    "                outputs[k] = node(*[outputs[x] for x in ins])\n",
    "        return outputs\n",
    "    \n",
    "    def half(self):\n",
    "        for node in self.nodes():\n",
    "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
    "                node.half()\n",
    "        return self\n",
    "\n",
    "class Identity(namedtuple('Identity', [])):\n",
    "    def __call__(self, x): return x\n",
    "\n",
    "class Add(namedtuple('Add', [])):\n",
    "    def __call__(self, x, y): return x + y \n",
    "    \n",
    "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
    "    def __call__(self, x, y): return self.wx*x + self.wy*y \n",
    "\n",
    "class Mul(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    def __call__(self, x): \n",
    "        return x*self.weight\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    def forward(self, *xs): return torch.cat(xs, 1)\n",
    "\n",
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight_freeze=False, bias_freeze=False, weight_init=1.0, bias_init=0.0):\n",
    "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
    "        if weight_init is not None: self.weight.data.fill_(weight_init)\n",
    "        if bias_init is not None: self.bias.data.fill_(bias_init)\n",
    "        self.weight.requires_grad = not weight_freeze\n",
    "        self.bias.requires_grad = not bias_freeze\n",
    "\n",
    "class GhostBatchNorm(BatchNorm):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "        self.num_splits = num_splits\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
    "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        if (self.training is True) and (mode is False): #lazily collate stats when we are going to use them\n",
    "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "        return super().train(mode)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            return nn.functional.batch_norm(\n",
    "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W) \n",
    "        else:\n",
    "            return nn.functional.batch_norm(\n",
    "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)\n",
    "\n",
    "# Losses\n",
    "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
    "    def __call__(self, log_probs, target):\n",
    "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
    "    \n",
    "class KLLoss(namedtuple('KLLoss', [])):        \n",
    "    def __call__(self, log_probs):\n",
    "        return -log_probs.mean(dim=1)\n",
    "\n",
    "class Correct(namedtuple('Correct', [])):\n",
    "    def __call__(self, classifier, target):\n",
    "        return classifier.max(dim = 1)[1] == target\n",
    "\n",
    "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
    "    def __call__(self, x):\n",
    "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
    "\n",
    "x_ent_loss = Network({\n",
    "  'loss':  (nn.CrossEntropyLoss(reduction='none'), ['logits', 'target']),\n",
    "  'acc': (Correct(), ['logits', 'target'])\n",
    "})\n",
    "\n",
    "label_smoothing_loss = lambda alpha: Network({\n",
    "        'logprobs': (LogSoftmax(dim=1), ['logits']),\n",
    "        'KL':  (KLLoss(), ['logprobs']),\n",
    "        'xent':  (CrossEntropyLoss(), ['logprobs', 'target']),\n",
    "        'loss': (AddWeighted(wx=1-alpha, wy=alpha), ['xent', 'KL']),\n",
    "        'acc': (Correct(), ['logits', 'target']),\n",
    "    })\n",
    "\n",
    "trainable_params = lambda model: {k:p for k,p in model.named_parameters() if p.requires_grad}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimisers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from functools import partial\n",
    "\n",
    "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
    "    dw.add_(weight_decay, w).mul_(-lr)\n",
    "    v.mul_(momentum).add_(dw)\n",
    "    w.add_(dw.add_(momentum, v))\n",
    "\n",
    "norm = lambda x: torch.norm(x.reshape(x.size(0),-1).float(), dim=1)[:,None,None,None]\n",
    "\n",
    "def LARS_update(w, dw, v, lr, weight_decay, momentum):\n",
    "    nesterov_update(w, dw, v, lr*(norm(w)/(norm(dw)+1e-2)).to(w.dtype), weight_decay, momentum)\n",
    "\n",
    "def zeros_like(weights):\n",
    "    return [torch.zeros_like(w) for w in weights]\n",
    "\n",
    "def optimiser(weights, param_schedule, update, state_init):\n",
    "    weights = list(weights)\n",
    "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
    "\n",
    "def opt_step(update, param_schedule, step_number, weights, opt_state):\n",
    "    step_number += 1\n",
    "    param_values = {k: f(step_number) for k, f in param_schedule.items()}\n",
    "    for w, v in zip(weights, opt_state):\n",
    "        if w.requires_grad:\n",
    "            update(w.data, w.grad.data, v, **param_values)\n",
    "    return {'update': update, 'param_schedule': param_schedule, 'step_number': step_number, 'weights': weights,  'opt_state': opt_state}\n",
    "\n",
    "LARS = partial(optimiser, update=LARS_update, state_init=zeros_like)\n",
    "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from itertools import chain\n",
    "\n",
    "def reduce(batches, state, steps):\n",
    "    #state: is a dictionary\n",
    "    #steps: are functions that take (batch, state)\n",
    "    #and return a dictionary of updates to the state (or None)\n",
    "    \n",
    "    for batch in chain(batches, [None]): \n",
    "    #we send an extra batch=None at the end for steps that \n",
    "    #need to do some tidying-up (e.g. log_activations)\n",
    "        for step in steps:\n",
    "            updates = step(batch, state)\n",
    "            if updates:\n",
    "                for k,v in updates.items():\n",
    "                    state[k] = v                  \n",
    "    return state\n",
    "  \n",
    "#define keys in the state dict as constants\n",
    "MODEL = 'model'\n",
    "LOSS = 'loss'\n",
    "VALID_MODEL = 'valid_model'\n",
    "OUTPUT = 'output'\n",
    "OPTS = 'optimisers'\n",
    "ACT_LOG = 'activation_log'\n",
    "WEIGHT_LOG = 'weight_log'\n",
    "\n",
    "#step definitions\n",
    "def forward(training_mode):\n",
    "    def step(batch, state):\n",
    "        if not batch: return\n",
    "        model = state[MODEL] if training_mode or (VALID_MODEL not in state) else state[VALID_MODEL]\n",
    "        if model.training != training_mode: #without the guard it's slow!\n",
    "            model.train(training_mode)\n",
    "        return {OUTPUT: state[LOSS](model(batch))}\n",
    "    return step\n",
    "\n",
    "def backward(dtype=None):\n",
    "    def step(batch, state):\n",
    "        state[MODEL].zero_grad()\n",
    "        if not batch: return\n",
    "        loss = state[OUTPUT][LOSS]\n",
    "        if dtype is not None:\n",
    "            loss = loss.to(dtype)\n",
    "        loss.sum().backward()\n",
    "    return step\n",
    "\n",
    "def opt_steps(batch, state):\n",
    "    if not batch: return\n",
    "    return {OPTS: [opt_step(**opt) for opt in state[OPTS]]}\n",
    "\n",
    "def log_activations(node_names=('loss', 'acc')):\n",
    "    def step(batch, state):\n",
    "        if '_tmp_logs_' not in state: \n",
    "            state['_tmp_logs_'] = []\n",
    "        if batch:\n",
    "            state['_tmp_logs_'].extend((k, state[OUTPUT][k].detach()) for k in node_names)\n",
    "        else:\n",
    "            res = {k: to_numpy(torch.cat(xs)).astype(float) for k, xs in group_by_key(state['_tmp_logs_']).items()}\n",
    "            del state['_tmp_logs_']\n",
    "            return {ACT_LOG: res}\n",
    "    return step\n",
    "\n",
    "epoch_stats = lambda state: {k: np.mean(v) for k, v in state[ACT_LOG].items()}\n",
    "\n",
    "default_train_steps = (forward(training_mode=True), log_activations(('loss', 'acc')), backward(), opt_steps)\n",
    "default_valid_steps = (forward(training_mode=False), log_activations(('loss', 'acc')))\n",
    "\n",
    "\n",
    "def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n",
    "                on_epoch_end=(lambda state: state)):\n",
    "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
    "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(include_in_total=False) #DAWNBench rules\n",
    "    return {\n",
    "        'train': union({'time': train_time}, train_summary), \n",
    "        'valid': union({'time': valid_time}, valid_summary), \n",
    "        'total time': timer.total_time\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def conv_bn(c_in, c_out):\n",
    "    return {\n",
    "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
    "        'bn': BatchNorm(c_out), \n",
    "        'relu': nn.ReLU(True)\n",
    "    }\n",
    "\n",
    "def residual(c):\n",
    "    return {\n",
    "        'in': Identity(),\n",
    "        'res1': conv_bn(c, c),\n",
    "        'res2': conv_bn(c, c),\n",
    "        'add': (Add(), ['in', 'res2/relu']),\n",
    "    }\n",
    "\n",
    "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3')):\n",
    "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
    "    n = {\n",
    "        'input': (None, []),\n",
    "        'prep': conv_bn(3, channels['prep']),\n",
    "        'layer1': dict(conv_bn(channels['prep'], channels['layer1']), pool=pool),\n",
    "        'layer2': dict(conv_bn(channels['layer1'], channels['layer2']), pool=pool),\n",
    "        'layer3': dict(conv_bn(channels['layer2'], channels['layer3']), pool=pool),\n",
    "        'pool': nn.MaxPool2d(4),\n",
    "        'flatten': Flatten(),\n",
    "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
    "        'logits': Mul(weight),\n",
    "    }\n",
    "    for layer in res_layers:\n",
    "        n[layer]['residual'] = residual(channels[layer])\n",
    "    for layer in extra_layers:\n",
    "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
    "    return n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download and preprocess data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "DATA_DIR = './data'\n",
    "dataset = cifar10(root=DATA_DIR)\n",
    "timer = Timer()\n",
    "\n",
    "transforms = [\n",
    "    partial(normalise, mean=np.array(cifar10_mean, dtype=np.float32), std=np.array(cifar10_std, dtype=np.float32)),\n",
    "    partial(transpose, source='NHWC', target='NCHW'), \n",
    "]\n",
    "\n",
    "train_set = list(zip(*preprocess(dataset['train'], [partial(pad, border=4)] + transforms).values()))\n",
    "valid_set = list(zip(*preprocess(dataset['valid'], transforms).values()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "epochs=24\n",
    "lr_schedule = PiecewiseLinear([0, 5, epochs], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "train_transforms = [Crop(32, 32), FlipLR(), Cutout(8, 8)]\n",
    "N_runs = 3\n",
    "\n",
    "train_batches = DataLoader(Transform(train_set, train_transforms), batch_size, shuffle=True, set_random_choices=True, drop_last=True)\n",
    "valid_batches = DataLoader(valid_set, batch_size, shuffle=False, drop_last=False)\n",
    "lr = lambda step: lr_schedule(step/len(train_batches))/batch_size\n",
    "\n",
    "summaries = []\n",
    "for i in range(N_runs):\n",
    "    print(f'Starting Run {i} at {localtime()}')\n",
    "    model = Network(net()).to(device).half()\n",
    "    opts = [SGD(trainable_params(model).values(), {'lr': lr, 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)})]\n",
    "    logs, state = Table(), {MODEL: model, LOSS: x_ent_loss, OPTS: opts}\n",
    "    for epoch in range(epochs):\n",
    "        logs.append(union({'epoch': epoch+1}, train_epoch(state, Timer(torch.cuda.synchronize), train_batches, valid_batches)))\n",
    "logs.df().query(f'epoch=={epochs}')[['train_acc', 'valid_acc']].describe()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Run 0 at 2022-05-09 05:52:47\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\AJ\\AppData\\Local\\Temp\\ipykernel_11056\\266132482.py:4: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1055.)\n",
      "  dw.add_(weight_decay, w).mul_(-lr)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1      10.8930       1.6436       0.4049       0.7421       1.3485       0.5272      10.8930\n",
      "           2       7.1690       0.9541       0.6599       0.4681       0.8276       0.7097       7.1690\n",
      "           3       7.3133       0.7327       0.7436       0.4678       0.8419       0.7199       7.3133\n",
      "           4       7.2487       0.6319       0.7836       0.4695       0.6619       0.7716       7.2487\n",
      "           5       7.1932       0.5605       0.8060       0.4466       0.7571       0.7501       7.1932\n",
      "           6       7.3533       0.4960       0.8301       0.4693       0.5311       0.8154       7.3533\n",
      "           7       7.2941       0.4434       0.8476       0.4695       0.5461       0.8137       7.2941\n",
      "           8       7.3393       0.4124       0.8588       0.4703       0.4860       0.8337       7.3393\n",
      "           9       7.3232       0.3875       0.8673       0.4716       0.4252       0.8598       7.3232\n",
      "          10       7.3596       0.3625       0.8779       0.4694       0.4209       0.8548       7.3596\n",
      "          11       7.3611       0.3476       0.8821       0.4700       0.4076       0.8608       7.3611\n",
      "          12       7.4954       0.3282       0.8875       0.4836       0.5684       0.8146       7.4954\n",
      "          13       7.5221       0.3075       0.8959       0.4872       0.4207       0.8573       7.5221\n",
      "          14       7.3633       0.2880       0.9034       0.4707       0.3985       0.8648       7.3633\n",
      "          15       7.3800       0.2724       0.9075       0.4723       0.4400       0.8534       7.3800\n",
      "          16       7.3783       0.2530       0.9129       0.4725       0.3576       0.8803       7.3783\n",
      "          17       7.3219       0.2320       0.9205       0.4736       0.3402       0.8868       7.3219\n",
      "          18       7.3205       0.2166       0.9275       0.4734       0.4728       0.8422       7.3205\n",
      "          19       7.3532       0.1911       0.9364       0.4721       0.2694       0.9083       7.3532\n",
      "          20       7.2699       0.1667       0.9452       0.4997       0.2875       0.9061       7.2699\n",
      "          21       7.4025       0.1431       0.9530       0.4737       0.2517       0.9148       7.4025\n",
      "          22       7.0751       0.1167       0.9634       0.4495       0.2128       0.9293       7.0751\n",
      "          23       7.1933       0.0944       0.9719       0.4715       0.1923       0.9368       7.1933\n",
      "          24       7.3993       0.0766       0.9785       0.4537       0.1807       0.9401       7.3993\n",
      "Starting Run 1 at 2022-05-09 05:55:58\n",
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       7.2445       1.6319       0.4105       0.4934       1.7572       0.4438       7.2445\n",
      "           2       7.2300       0.9426       0.6644       0.5021       0.9789       0.6674       7.2300\n",
      "           3       7.3317       0.7292       0.7453       0.4680       0.8256       0.7300       7.3317\n",
      "           4       7.4228       0.6251       0.7848       0.4752       0.7859       0.7322       7.4228\n",
      "           5       7.3869       0.5544       0.8074       0.4527       0.5921       0.7976       7.3869\n",
      "           6       7.3508       0.5039       0.8246       0.4745       0.5958       0.7964       7.3508\n",
      "           7       7.2788       0.4444       0.8465       0.4747       0.5030       0.8318       7.2788\n",
      "           8       7.5033       0.4104       0.8602       0.4764       0.5104       0.8216       7.5033\n",
      "           9       7.4222       0.3893       0.8667       0.4735       0.4509       0.8439       7.4222\n",
      "          10       7.4411       0.3642       0.8758       0.4744       0.5503       0.8172       7.4411\n",
      "          11       7.3405       0.3430       0.8832       0.4750       0.4453       0.8493       7.3405\n",
      "          12       7.3361       0.3305       0.8865       0.4825       0.4807       0.8402       7.3361\n",
      "          13       7.3951       0.3080       0.8940       0.4730       0.4172       0.8603       7.3951\n",
      "          14       7.3606       0.2857       0.9031       0.4686       0.4730       0.8458       7.3606\n",
      "          15       7.3424       0.2715       0.9076       0.4737       0.3512       0.8839       7.3424\n",
      "          16       7.3857       0.2520       0.9139       0.4737       0.4431       0.8547       7.3857\n",
      "          17       7.3658       0.2317       0.9212       0.4742       0.3165       0.8933       7.3658\n",
      "          18       7.2895       0.2111       0.9285       0.4726       0.3726       0.8809       7.2895\n",
      "          19       7.3587       0.1915       0.9358       0.4727       0.3070       0.8966       7.3587\n",
      "          20       7.3675       0.1638       0.9470       0.4725       0.2662       0.9091       7.3675\n",
      "          21       7.1534       0.1423       0.9532       0.4720       0.2376       0.9197       7.1534\n",
      "          22       7.3698       0.1167       0.9643       0.4721       0.2082       0.9323       7.3698\n",
      "          23       7.2247       0.0958       0.9706       0.4730       0.1931       0.9378       7.2247\n",
      "          24       7.3637       0.0764       0.9784       0.4537       0.1786       0.9423       7.3637\n",
      "Starting Run 2 at 2022-05-09 05:59:06\n",
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       7.1657       1.6313       0.4102       0.4736       1.3377       0.5409       7.1657\n",
      "           2       7.3924       0.9368       0.6677       0.4738       1.0531       0.6381       7.3924\n",
      "           3       7.3704       0.7238       0.7454       0.4735       0.7321       0.7487       7.3704\n",
      "           4       7.4055       0.6338       0.7792       0.4747       0.6053       0.7907       7.4055\n",
      "           5       7.3453       0.5561       0.8078       0.4738       0.6596       0.7729       7.3453\n",
      "           6       7.3882       0.4990       0.8282       0.4736       0.5514       0.8094       7.3882\n",
      "           7       7.3904       0.4442       0.8479       0.4749       0.4746       0.8384       7.3904\n",
      "           8       7.3882       0.4146       0.8588       0.4733       0.5788       0.8066       7.3882\n",
      "           9       7.3824       0.3903       0.8672       0.4657       0.5972       0.7889       7.3824\n",
      "          10       7.3817       0.3651       0.8747       0.4733       0.4318       0.8495       7.3817\n",
      "          11       7.3446       0.3438       0.8832       0.4608       0.4073       0.8614       7.3446\n",
      "          12       7.2380       0.3257       0.8886       0.4750       0.4088       0.8639       7.2380\n",
      "          13       7.3418       0.3085       0.8941       0.4745       0.4103       0.8599       7.3418\n",
      "          14       7.3803       0.2926       0.8994       0.4703       0.4052       0.8635       7.3803\n",
      "          15       7.3764       0.2721       0.9071       0.4749       0.3369       0.8860       7.3764\n",
      "          16       7.2289       0.2547       0.9135       0.4509       0.3479       0.8815       7.2289\n",
      "          17       7.2567       0.2329       0.9203       0.4732       0.3434       0.8855       7.2567\n",
      "          18       7.3798       0.2104       0.9300       0.4734       0.3062       0.8975       7.3798\n",
      "          19       7.3286       0.1895       0.9373       0.4496       0.2757       0.9042       7.3286\n",
      "          20       7.3710       0.1661       0.9450       0.4560       0.2969       0.9035       7.3710\n",
      "          21       7.3366       0.1410       0.9542       0.4739       0.2246       0.9249       7.3366\n",
      "          22       7.2923       0.1181       0.9626       0.4724       0.2302       0.9230       7.2923\n",
      "          23       7.3816       0.0940       0.9718       0.4749       0.1950       0.9352       7.3816\n",
      "          24       7.3780       0.0751       0.9788       0.4733       0.1774       0.9413       7.3780\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       train_acc  valid_acc\n",
       "count   1.000000     1.0000\n",
       "mean    0.978818     0.9413\n",
       "std          NaN        NaN\n",
       "min     0.978818     0.9413\n",
       "25%     0.978818     0.9413\n",
       "50%     0.978818     0.9413\n",
       "75%     0.978818     0.9413\n",
       "max     0.978818     0.9413"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.978818</td>\n",
       "      <td>0.9413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.978818</td>\n",
       "      <td>0.9413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.978818</td>\n",
       "      <td>0.9413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.978818</td>\n",
       "      <td>0.9413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.978818</td>\n",
       "      <td>0.9413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.978818</td>\n",
       "      <td>0.9413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}