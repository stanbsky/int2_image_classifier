{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "import copy\n",
    "from collections import namedtuple, defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import singledispatch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-corrections",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sound-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self, synch=None):\n",
    "        self.synch = synch or (lambda: None)\n",
    "        self.synch()\n",
    "        self.times = [time.perf_counter()]\n",
    "        self.total_time = 0.0\n",
    "\n",
    "    def __call__(self, include_in_total=True):\n",
    "        self.synch()\n",
    "        self.times.append(time.perf_counter())\n",
    "        delta_t = self.times[-1] - self.times[-2]\n",
    "        if include_in_total:\n",
    "            self.total_time += delta_t\n",
    "        return delta_t\n",
    "    \n",
    "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "default_table_formats = {float: '{:{w}.4f}', str: '{:>{w}s}', 'default': '{:{w}}', 'title': '{:>{w}s}'}\n",
    "\n",
    "def table_formatter(val, is_title=False, col_width=12, formats=None):\n",
    "    formats = formats or default_table_formats\n",
    "    type_ = lambda val: float if isinstance(val, (float, float)) else type(val)\n",
    "    return (formats['title'] if is_title else formats.get(type_(val), formats['default'])).format(val, w=col_width)\n",
    "\n",
    "class Table():\n",
    "    def __init__(self, keys=None, report=(lambda data: True), formatter=table_formatter):\n",
    "        self.keys, self.report, self.formatter = keys, report, formatter\n",
    "        self.log = []\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.log.append(data)\n",
    "        data = {' '.join(p): v for p,v in path_iter(data)}\n",
    "        self.keys = self.keys or data.keys()\n",
    "        if len(self.log) == 1:\n",
    "            print(*(self.formatter(k, True) for k in self.keys))\n",
    "        if self.report(data):\n",
    "            print(*(self.formatter(data[k]) for k in self.keys))\n",
    "            \n",
    "    def df(self):\n",
    "        return pd.DataFrame([{'_'.join(p): v for p,v in path_iter(row)} for row in self.log])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-developer",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "related-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, transforms):\n",
    "    dataset = copy.copy(dataset) #shallow copy\n",
    "    for transform in transforms:\n",
    "        dataset['data'] = transform(dataset['data'])\n",
    "    return dataset\n",
    "\n",
    "@singledispatch\n",
    "def normalise(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "@normalise.register(np.ndarray) \n",
    "def _(x, mean, std): \n",
    "    #faster inplace for numpy arrays\n",
    "    x = np.array(x, np.float32)\n",
    "    x -= mean\n",
    "    x *= 1.0/std\n",
    "    return x\n",
    "\n",
    "@singledispatch\n",
    "def pad(x, border):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@pad.register(np.ndarray)\n",
    "def _(x, border): \n",
    "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
    "\n",
    "@singledispatch\n",
    "def transpose(x, source, target):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@transpose.register(np.ndarray)\n",
    "def _(x, source, target):\n",
    "    return x.transpose([source.index(d) for d in target]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-reward",
   "metadata": {},
   "source": [
    "## Dict utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prescription-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
    "\n",
    "def path_iter(nested_dict, pfx=()):\n",
    "    for name, val in nested_dict.items():\n",
    "        if isinstance(val, dict): yield from path_iter(val, (*pfx, name))\n",
    "        else: yield ((*pfx, name), val)  \n",
    "\n",
    "def map_nested(func, nested_dict):\n",
    "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k,v in nested_dict.items()}\n",
    "\n",
    "def group_by_key(items):\n",
    "    res = defaultdict(list)\n",
    "    for k, v in items: \n",
    "        res[k].append(v) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-turtle",
   "metadata": {},
   "source": [
    "## Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incoming-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = '/'\n",
    "\n",
    "def split(path):\n",
    "    i = path.rfind(sep) + 1\n",
    "    return path[:i].rstrip(sep), path[i:]\n",
    "\n",
    "def normpath(path):\n",
    "    #simplified os.path.normpath\n",
    "    parts = []\n",
    "    for p in path.split(sep):\n",
    "        if p == '..': parts.pop()\n",
    "        elif p.startswith(sep): parts = [p]\n",
    "        else: parts.append(p)\n",
    "    return sep.join(parts)\n",
    "\n",
    "has_inputs = lambda node: type(node) is tuple\n",
    "\n",
    "def pipeline(net):\n",
    "    return [(sep.join(path), (node if has_inputs(node) else (node, [-1]))) for (path, node) in path_iter(net)]\n",
    "\n",
    "def build_graph(net):\n",
    "    flattened = pipeline(net)\n",
    "    resolve_input = lambda rel_path, path, idx: normpath(sep.join((path, '..', rel_path))) if isinstance(rel_path, str) else flattened[idx+rel_path][0]\n",
    "    return {path: (node[0], [resolve_input(rel_path, path, idx) for rel_path in node[1]]) for idx, (path, node) in enumerate(flattened)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-stevens",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "final-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "@singledispatch\n",
    "def cat(*xs):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "@singledispatch\n",
    "def to_numpy(x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
    "    def __call__(self, t):\n",
    "        return np.interp([t], self.knots, self.vals)[0]\n",
    " \n",
    "class Const(namedtuple('Const', ['val'])):\n",
    "    def __call__(self, x):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-decade",
   "metadata": {},
   "source": [
    "## Pytorch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "supposed-serve",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flip_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-762b1c23c7bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mflip_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flip_lr' is not defined"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "@cat.register(torch.Tensor)\n",
    "def _(*xs):\n",
    "    return torch.cat(xs)\n",
    "\n",
    "@to_numpy.register(torch.Tensor)\n",
    "def _(x):\n",
    "    return x.detach().cpu().numpy()  \n",
    "\n",
    "@pad.register(torch.Tensor)\n",
    "def _(x, border):\n",
    "    return nn.ReflectionPad2d(border)(x)\n",
    "\n",
    "@transpose.register(torch.Tensor)\n",
    "def _(x, source, target):\n",
    "    return x.permute([source.index(d) for d in target]) \n",
    "\n",
    "def to(*args, **kwargs): \n",
    "    return lambda x: x.to(*args, **kwargs)\n",
    "\n",
    "@flip_lr.register(torch.Tensor)\n",
    "def _(x):\n",
    "    return torch.flip(x, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-hobby",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache as cache\n",
    "\n",
    "@cache(None)\n",
    "def cifar10(root='./data'):\n",
    "    try:\n",
    "        \n",
    "        download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
    "        return {k: {'data': v.data, 'targets': v.targets} for k,v in [('train', download(train=True)), ('valid', download(train=False))]}\n",
    "    except ImportError:\n",
    "        from tensorflow.keras import datasets\n",
    "        (train_images, train_labels), (valid_images, valid_labels) = datasets.cifar10.load_data()\n",
    "        return {\n",
    "            'train': {'data': train_images, 'targets': train_labels.squeeze()},\n",
    "            'valid': {'data': valid_images, 'targets': valid_labels.squeeze()}\n",
    "        }\n",
    "             \n",
    "cifar10_mean, cifar10_std = [\n",
    "    (125.31, 122.95, 113.87), # equals np.mean(cifar10()['train']['data'], axis=(0,1,2)) \n",
    "    (62.99, 62.09, 66.70), # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
    "]\n",
    "cifar10_classes= 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-portrait",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.set_random_choices = set_random_choices\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
    "        )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.set_random_choices:\n",
    "            self.dataset.set_random_choices() \n",
    "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-pencil",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.graph = build_graph(net)\n",
    "        for path, (val, _) in self.graph.items(): \n",
    "            setattr(self, path.replace('/', '_'), val)\n",
    "    \n",
    "    def nodes(self):\n",
    "        return (node for node, _ in self.graph.values())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = dict(inputs)\n",
    "        for k, (node, ins) in self.graph.items():\n",
    "            #only compute nodes that are not supplied as inputs.\n",
    "            if k not in outputs: \n",
    "                outputs[k] = node(*[outputs[x] for x in ins])\n",
    "        return outputs\n",
    "    \n",
    "    def half(self):\n",
    "        for node in self.nodes():\n",
    "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
    "                node.half()\n",
    "        return self\n",
    "\n",
    "class Identity(namedtuple('Identity', [])):\n",
    "    def __call__(self, x): return x\n",
    "\n",
    "class Add(namedtuple('Add', [])):\n",
    "    def __call__(self, x, y): return x + y \n",
    "    \n",
    "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
    "    def __call__(self, x, y): return self.wx*x + self.wy*y \n",
    "\n",
    "class Mul(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    def __call__(self, x): \n",
    "        return x*self.weight\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    def forward(self, *xs): return torch.cat(xs, 1)\n",
    "\n",
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight_freeze=False, bias_freeze=False, weight_init=1.0, bias_init=0.0):\n",
    "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
    "        if weight_init is not None: self.weight.data.fill_(weight_init)\n",
    "        if bias_init is not None: self.bias.data.fill_(bias_init)\n",
    "        self.weight.requires_grad = not weight_freeze\n",
    "        self.bias.requires_grad = not bias_freeze\n",
    "        \n",
    "# Losses\n",
    "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
    "    def __call__(self, log_probs, target):\n",
    "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
    "    \n",
    "class KLLoss(namedtuple('KLLoss', [])):        \n",
    "    def __call__(self, log_probs):\n",
    "        return -log_probs.mean(dim=1)\n",
    "\n",
    "class Correct(namedtuple('Correct', [])):\n",
    "    def __call__(self, classifier, target):\n",
    "        return classifier.max(dim = 1)[1] == target\n",
    "\n",
    "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
    "    def __call__(self, x):\n",
    "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
    "\n",
    "x_ent_loss = Network({\n",
    "  'loss':  (nn.CrossEntropyLoss(reduction='none'), ['logits', 'target']),\n",
    "  'acc': (Correct(), ['logits', 'target'])\n",
    "})\n",
    "\n",
    "label_smoothing_loss = lambda alpha: Network({\n",
    "        'logprobs': (LogSoftmax(dim=1), ['logits']),\n",
    "        'KL':  (KLLoss(), ['logprobs']),\n",
    "        'xent':  (CrossEntropyLoss(), ['logprobs', 'target']),\n",
    "        'loss': (AddWeighted(wx=1-alpha, wy=alpha), ['xent', 'KL']),\n",
    "        'acc': (Correct(), ['logits', 'target']),\n",
    "    })\n",
    "\n",
    "trainable_params = lambda model: {k:p for k,p in model.named_parameters() if p.requires_grad}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-hollywood",
   "metadata": {},
   "source": [
    "## Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
    "    dw.add_(weight_decay, w).mul_(-lr)\n",
    "    v.mul_(momentum).add_(dw)\n",
    "    w.add_(dw.add_(momentum, v))\n",
    "\n",
    "norm = lambda x: torch.norm(x.reshape(x.size(0),-1).float(), dim=1)[:,None,None,None]\n",
    "\n",
    "def LARS_update(w, dw, v, lr, weight_decay, momentum):\n",
    "    nesterov_update(w, dw, v, lr*(norm(w)/(norm(dw)+1e-2)).to(w.dtype), weight_decay, momentum)\n",
    "\n",
    "def zeros_like(weights):\n",
    "    return [torch.zeros_like(w) for w in weights]\n",
    "\n",
    "def optimiser(weights, param_schedule, update, state_init):\n",
    "    weights = list(weights)\n",
    "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
    "\n",
    "def opt_step(update, param_schedule, step_number, weights, opt_state):\n",
    "    step_number += 1\n",
    "    param_values = {k: f(step_number) for k, f in param_schedule.items()}\n",
    "    for w, v in zip(weights, opt_state):\n",
    "        if w.requires_grad:\n",
    "            update(w.data, w.grad.data, v, **param_values)\n",
    "    return {'update': update, 'param_schedule': param_schedule, 'step_number': step_number, 'weights': weights,  'opt_state': opt_state}\n",
    "\n",
    "LARS = partial(optimiser, update=LARS_update, state_init=zeros_like)\n",
    "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-shame",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def reduce(batches, state, steps):\n",
    "    #state: is a dictionary\n",
    "    #steps: are functions that take (batch, state)\n",
    "    #and return a dictionary of updates to the state (or None)\n",
    "    \n",
    "    for batch in chain(batches, [None]): \n",
    "    #we send an extra batch=None at the end for steps that \n",
    "    #need to do some tidying-up (e.g. log_activations)\n",
    "        for step in steps:\n",
    "            updates = step(batch, state)\n",
    "            if updates:\n",
    "                for k,v in updates.items():\n",
    "                    state[k] = v                  \n",
    "    return state\n",
    "  \n",
    "#define keys in the state dict as constants\n",
    "MODEL = 'model'\n",
    "LOSS = 'loss'\n",
    "VALID_MODEL = 'valid_model'\n",
    "OUTPUT = 'output'\n",
    "OPTS = 'optimisers'\n",
    "ACT_LOG = 'activation_log'\n",
    "WEIGHT_LOG = 'weight_log'\n",
    "\n",
    "#step definitions\n",
    "def forward(training_mode):\n",
    "    def step(batch, state):\n",
    "        if not batch: return\n",
    "        model = state[MODEL] if training_mode or (VALID_MODEL not in state) else state[VALID_MODEL]\n",
    "        if model.training != training_mode: #without the guard it's slow!\n",
    "            model.train(training_mode)\n",
    "        return {OUTPUT: state[LOSS](model(batch))}\n",
    "    return step\n",
    "\n",
    "def backward(dtype=None):\n",
    "    def step(batch, state):\n",
    "        state[MODEL].zero_grad()\n",
    "        if not batch: return\n",
    "        loss = state[OUTPUT][LOSS]\n",
    "        if dtype is not None:\n",
    "            loss = loss.to(dtype)\n",
    "        loss.sum().backward()\n",
    "    return step\n",
    "\n",
    "def opt_steps(batch, state):\n",
    "    if not batch: return\n",
    "    return {OPTS: [opt_step(**opt) for opt in state[OPTS]]}\n",
    "\n",
    "def log_activations(node_names=('loss', 'acc')):\n",
    "    def step(batch, state):\n",
    "        if '_tmp_logs_' not in state: \n",
    "            state['_tmp_logs_'] = []\n",
    "        if batch:\n",
    "            state['_tmp_logs_'].extend((k, state[OUTPUT][k].detach()) for k in node_names)\n",
    "        else:\n",
    "            res = {k: to_numpy(torch.cat(xs)).astype(float) for k, xs in group_by_key(state['_tmp_logs_']).items()}\n",
    "            del state['_tmp_logs_']\n",
    "            return {ACT_LOG: res}\n",
    "    return step\n",
    "\n",
    "epoch_stats = lambda state: {k: np.mean(v) for k, v in state[ACT_LOG].items()}\n",
    "\n",
    "default_train_steps = (forward(training_mode=True), log_activations(('loss', 'acc')), backward(), opt_steps)\n",
    "default_valid_steps = (forward(training_mode=False), log_activations(('loss', 'acc')))\n",
    "\n",
    "\n",
    "def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n",
    "                on_epoch_end=(lambda state: state)):\n",
    "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
    "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(include_in_total=False) #DAWNBench rules\n",
    "    return {\n",
    "        'train': union({'time': train_time}, train_summary), \n",
    "        'valid': union({'time': valid_time}, valid_summary), \n",
    "        'total time': timer.total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-stewart",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(c_in, c_out):\n",
    "    return {\n",
    "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
    "        'bn': BatchNorm(c_out), \n",
    "        'relu': nn.ReLU(True)\n",
    "    }\n",
    "\n",
    "def residual(c):\n",
    "    return {\n",
    "        'in': Identity(),\n",
    "        'res1': conv_bn(c, c),\n",
    "        'res2': conv_bn(c, c),\n",
    "        'add': (Add(), ['in', 'res2/relu']),\n",
    "    }\n",
    "\n",
    "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3')):\n",
    "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
    "    n = {\n",
    "        'input': (None, []),\n",
    "        'prep': conv_bn(3, channels['prep']),\n",
    "        'layer1': dict(conv_bn(channels['prep'], channels['layer1']), pool=pool),\n",
    "        'layer2': dict(conv_bn(channels['layer1'], channels['layer2']), pool=pool),\n",
    "        'layer3': dict(conv_bn(channels['layer2'], channels['layer3']), pool=pool),\n",
    "        'pool': nn.MaxPool2d(4),\n",
    "        'flatten': Flatten(),\n",
    "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
    "        'logits': Mul(weight),\n",
    "    }\n",
    "    for layer in res_layers:\n",
    "        n[layer]['residual'] = residual(channels[layer])\n",
    "    for layer in extra_layers:\n",
    "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-tsunami",
   "metadata": {},
   "source": [
    "## Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = './data'\n",
    "# dataset = cifar10(root=DATA_DIR)\n",
    "timer = Timer()\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tt\n",
    "stats= ((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)) #mean and std\n",
    "train_tfm= tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), # transormation of data together\n",
    "                       tt.RandomHorizontalFlip(),\n",
    "                       tt.ToTensor(),tt.Normalize(*stats,inplace=True)])\n",
    "valid_tfm = tt.Compose([tt.ToTensor(),tt.Normalize(*stats)])\n",
    "\n",
    "train_set = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_tfm,\n",
    ")\n",
    "\n",
    "valid_set = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=valid_tfm,\n",
    ")\n",
    "\n",
    "# transforms = [\n",
    "#     partial(normalise, mean=np.array(cifar10_mean, dtype=np.float32), std=np.array(cifar10_std, dtype=np.float32)),\n",
    "#     partial(transpose, source='NHWC', target='NCHW'), \n",
    "# ]\n",
    "\n",
    "# train_set = list(zip(*preprocess(dataset['train'], [partial(pad, border=4)] + transforms).values()))\n",
    "# valid_set = list(zip(*preprocess(dataset['valid'], transforms).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-lincoln",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=24\n",
    "lr_schedule = PiecewiseLinear([0, 5, epochs], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "# train_transforms = [Crop(32, 32), FlipLR(), Cutout(8, 8)]\n",
    "N_runs = 3\n",
    "\n",
    "train_batches = DataLoader(train_set, batch_size, shuffle=True, set_random_choices=False, drop_last=True)\n",
    "valid_batches = DataLoader(valid_set, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# train_batches = DataLoader(Transform(train_set, train_transforms), batch_size, shuffle=True, set_random_choices=True, drop_last=True)\n",
    "# valid_batches = DataLoader(valid_set, batch_size, shuffle=False, drop_last=False)\n",
    "lr = lambda step: lr_schedule(step/len(train_batches))/batch_size\n",
    "\n",
    "summaries = []\n",
    "for i in range(N_runs):\n",
    "    print(f'Starting Run {i} at {localtime()}')\n",
    "    model = Network(net()).to(device).half()\n",
    "    opts = [SGD(trainable_params(model).values(), {'lr': lr, 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)})]\n",
    "    logs, state = Table(), {MODEL: model, LOSS: x_ent_loss, OPTS: opts}\n",
    "    for epoch in range(epochs):\n",
    "        logs.append(union({'epoch': epoch+1}, train_epoch(state, Timer(torch.cuda.synchronize), train_batches, valid_batches)))\n",
    "logs.df().query(f'epoch=={epochs}')[['train_acc', 'valid_acc']].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
