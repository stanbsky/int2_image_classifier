{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "visible-spectrum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.222408Z",
     "start_time": "2022-05-10T16:34:10.893424Z"
    }
   },
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "import copy\n",
    "from collections import namedtuple, defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import singledispatch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-guest",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deadly-collapse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.236041Z",
     "start_time": "2022-05-10T16:34:12.224744Z"
    }
   },
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self, synch=None):\n",
    "        self.synch = synch or (lambda: None)\n",
    "        self.synch()\n",
    "        self.times = [time.perf_counter()]\n",
    "        self.total_time = 0.0\n",
    "\n",
    "    def __call__(self, include_in_total=True):\n",
    "        self.synch()\n",
    "        self.times.append(time.perf_counter())\n",
    "        delta_t = self.times[-1] - self.times[-2]\n",
    "        if include_in_total:\n",
    "            self.total_time += delta_t\n",
    "        return delta_t\n",
    "    \n",
    "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "default_table_formats = {float: '{:{w}.4f}', str: '{:>{w}s}', 'default': '{:{w}}', 'title': '{:>{w}s}'}\n",
    "\n",
    "def table_formatter(val, is_title=False, col_width=12, formats=None):\n",
    "    formats = formats or default_table_formats\n",
    "    type_ = lambda val: float if isinstance(val, (float, float)) else type(val)\n",
    "    return (formats['title'] if is_title else formats.get(type_(val), formats['default'])).format(val, w=col_width)\n",
    "\n",
    "class Table():\n",
    "    def __init__(self, keys=None, report=(lambda data: True), formatter=table_formatter):\n",
    "        self.keys, self.report, self.formatter = keys, report, formatter\n",
    "        self.log = []\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.log.append(data)\n",
    "        data = {' '.join(p): v for p,v in path_iter(data)}\n",
    "        self.keys = self.keys or data.keys()\n",
    "        if len(self.log) == 1:\n",
    "            print(*(self.formatter(k, True) for k in self.keys))\n",
    "        if self.report(data):\n",
    "            print(*(self.formatter(data[k]) for k in self.keys))\n",
    "            \n",
    "    def df(self):\n",
    "        return pd.DataFrame([{'_'.join(p): v for p,v in path_iter(row)} for row in self.log])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-today",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpha-evening",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.250778Z",
     "start_time": "2022-05-10T16:34:12.238287Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(dataset, transforms):\n",
    "    dataset = copy.copy(dataset) #shallow copy\n",
    "    for transform in transforms:\n",
    "        dataset['data'] = transform(dataset['data'])\n",
    "    return dataset\n",
    "\n",
    "@singledispatch\n",
    "def normalise(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "@normalise.register(np.ndarray) \n",
    "def _(x, mean, std): \n",
    "    #faster inplace for numpy arrays\n",
    "    x = np.array(x, np.float32)\n",
    "    x -= mean\n",
    "    x *= 1.0/std\n",
    "    return x\n",
    "\n",
    "@singledispatch\n",
    "def pad(x, border):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@pad.register(np.ndarray)\n",
    "def _(x, border): \n",
    "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
    "\n",
    "@singledispatch\n",
    "def transpose(x, source, target):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@transpose.register(np.ndarray)\n",
    "def _(x, source, target):\n",
    "    return x.transpose([source.index(d) for d in target]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-david",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imposed-intelligence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.273236Z",
     "start_time": "2022-05-10T16:34:12.252755Z"
    }
   },
   "outputs": [],
   "source": [
    "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
    "    def __call__(self, x, x0, y0):\n",
    "        return x[..., y0:y0+self.h, x0:x0+self.w]\n",
    "\n",
    "    def options(self, shape):\n",
    "        *_, H, W = shape\n",
    "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]\n",
    "    \n",
    "    def output_shape(self, shape):\n",
    "        *_, H, W = shape\n",
    "        return (*_, self.h, self.w)\n",
    "\n",
    "@singledispatch\n",
    "def flip_lr(x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@flip_lr.register(np.ndarray)\n",
    "def _(x): \n",
    "    return x[..., ::-1].copy()\n",
    "\n",
    "class FlipLR(namedtuple('FlipLR', ())):\n",
    "    def __call__(self, x, choice):\n",
    "        return flip_lr(x) if choice else x \n",
    "        \n",
    "    def options(self, shape):\n",
    "        return [{'choice': b} for b in [True, False]]\n",
    "\n",
    "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
    "    def __call__(self, x, x0, y0):\n",
    "        x[..., y0:y0+self.h, x0:x0+self.w] = 0.0\n",
    "        return x\n",
    "\n",
    "    def options(self, shape):\n",
    "        *_, H, W = shape\n",
    "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]    \n",
    "    \n",
    "\n",
    "class Transform():\n",
    "    def __init__(self, dataset, transforms):\n",
    "        self.dataset, self.transforms = dataset, transforms\n",
    "        self.choices = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "           \n",
    "    def __getitem__(self, index):\n",
    "        data, labels = self.dataset[index]\n",
    "        data = data.copy()\n",
    "        for choices, f in zip(self.choices, self.transforms):\n",
    "            data = f(data, **choices[index])\n",
    "        return data, labels\n",
    "    \n",
    "    def set_random_choices(self):\n",
    "        self.choices = []\n",
    "        x_shape = self.dataset[0][0].shape\n",
    "        N = len(self)\n",
    "        for t in self.transforms:\n",
    "            self.choices.append(np.random.choice(t.options(x_shape), N))\n",
    "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-liechtenstein",
   "metadata": {},
   "source": [
    "## Dict utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worldwide-client",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.285870Z",
     "start_time": "2022-05-10T16:34:12.274762Z"
    }
   },
   "outputs": [],
   "source": [
    "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
    "\n",
    "def path_iter(nested_dict, pfx=()):\n",
    "    for name, val in nested_dict.items():\n",
    "        if isinstance(val, dict): yield from path_iter(val, (*pfx, name))\n",
    "        else: yield ((*pfx, name), val)  \n",
    "\n",
    "def map_nested(func, nested_dict):\n",
    "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k,v in nested_dict.items()}\n",
    "\n",
    "def group_by_key(items):\n",
    "    res = defaultdict(list)\n",
    "    for k, v in items: \n",
    "        res[k].append(v) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-synthesis",
   "metadata": {},
   "source": [
    "## Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "temporal-hampshire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.302470Z",
     "start_time": "2022-05-10T16:34:12.287344Z"
    }
   },
   "outputs": [],
   "source": [
    "sep = '/'\n",
    "\n",
    "def split(path):\n",
    "    i = path.rfind(sep) + 1\n",
    "    return path[:i].rstrip(sep), path[i:]\n",
    "\n",
    "def normpath(path):\n",
    "    #simplified os.path.normpath\n",
    "    parts = []\n",
    "    for p in path.split(sep):\n",
    "        if p == '..': parts.pop()\n",
    "        elif p.startswith(sep): parts = [p]\n",
    "        else: parts.append(p)\n",
    "    return sep.join(parts)\n",
    "\n",
    "has_inputs = lambda node: type(node) is tuple\n",
    "\n",
    "def pipeline(net):\n",
    "    return [(sep.join(path), (node if has_inputs(node) else (node, [-1]))) for (path, node) in path_iter(net)]\n",
    "\n",
    "def build_graph(net):\n",
    "    flattened = pipeline(net)\n",
    "    resolve_input = lambda rel_path, path, idx: normpath(sep.join((path, '..', rel_path))) if isinstance(rel_path, str) else flattened[idx+rel_path][0]\n",
    "    return {path: (node[0], [resolve_input(rel_path, path, idx) for rel_path in node[1]]) for idx, (path, node) in enumerate(flattened)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-tours",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "romantic-google",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.317308Z",
     "start_time": "2022-05-10T16:34:12.304555Z"
    }
   },
   "outputs": [],
   "source": [
    "@singledispatch\n",
    "def cat(*xs):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "@singledispatch\n",
    "def to_numpy(x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
    "    def __call__(self, t):\n",
    "        return np.interp([t], self.knots, self.vals)[0]\n",
    " \n",
    "class Const(namedtuple('Const', ['val'])):\n",
    "    def __call__(self, x):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-briefing",
   "metadata": {},
   "source": [
    "## Pytorch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "timely-thesis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.365106Z",
     "start_time": "2022-05-10T16:34:12.319871Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "@cat.register(torch.Tensor)\n",
    "def _(*xs):\n",
    "    return torch.cat(xs)\n",
    "\n",
    "@to_numpy.register(torch.Tensor)\n",
    "def _(x):\n",
    "    return x.detach().cpu().numpy()  \n",
    "\n",
    "@pad.register(torch.Tensor)\n",
    "def _(x, border):\n",
    "    return nn.ReflectionPad2d(border)(x)\n",
    "\n",
    "@transpose.register(torch.Tensor)\n",
    "def _(x, source, target):\n",
    "    return x.permute([source.index(d) for d in target]) \n",
    "\n",
    "def to(*args, **kwargs): \n",
    "    return lambda x: x.to(*args, **kwargs)\n",
    "\n",
    "@flip_lr.register(torch.Tensor)\n",
    "def _(x):\n",
    "    return torch.flip(x, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-drill",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accredited-marking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:12.377781Z",
     "start_time": "2022-05-10T16:34:12.367132Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache as cache\n",
    "\n",
    "@cache(None)\n",
    "def cifar10(root='./data'):\n",
    "    try:\n",
    "        \n",
    "        download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
    "        return {k: {'data': v.data, 'targets': v.targets} for k,v in [('train', download(train=True)), ('valid', download(train=False))]}\n",
    "    except ImportError:\n",
    "        from tensorflow.keras import datasets\n",
    "        (train_images, train_labels), (valid_images, valid_labels) = datasets.cifar10.load_data()\n",
    "        return {\n",
    "            'train': {'data': train_images, 'targets': train_labels.squeeze()},\n",
    "            'valid': {'data': valid_images, 'targets': valid_labels.squeeze()}\n",
    "        }\n",
    "             \n",
    "cifar10_mean, cifar10_std = [\n",
    "    (125.31, 122.95, 113.87), # equals np.mean(cifar10()['train']['data'], axis=(0,1,2)) \n",
    "    (62.99, 62.09, 66.70), # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
    "]\n",
    "cifar10_classes= 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "arranged-burst",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:14.452756Z",
     "start_time": "2022-05-10T16:34:12.379582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([125.30691805, 122.95039414, 113.86538318])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cifar10()['train']['data'], axis=(0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crucial-discrimination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:38:28.562263Z",
     "start_time": "2022-05-10T16:38:28.554726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10()['train']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "invalid-greece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:34:15.365350Z",
     "start_time": "2022-05-10T16:34:14.454616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.99321928, 62.08870764, 66.70489964])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cifar10()['train']['data'], axis=(0,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-lodge",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "living-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.set_random_choices = set_random_choices\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
    "        )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.set_random_choices:\n",
    "            self.dataset.set_random_choices() \n",
    "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.dataloader)\n",
    "    \n",
    "#GPU dataloading\n",
    "chunks = lambda data, splits: (data[start:end] for (start, end) in zip(splits, splits[1:]))\n",
    "\n",
    "even_splits = lambda N, num_chunks: np.cumsum([0] + [(N//num_chunks)+1]*(N % num_chunks)  + [N//num_chunks]*(num_chunks - (N % num_chunks)))\n",
    "\n",
    "def shuffled(xs, inplace=False):\n",
    "    xs = xs if inplace else copy.copy(xs) \n",
    "    np.random.shuffle(xs)\n",
    "    return xs\n",
    "\n",
    "def transformed(data, targets, transform, max_options=None, unshuffle=False):\n",
    "    i = torch.randperm(len(data), device=device)\n",
    "    data = data[i]\n",
    "    options = shuffled(transform.options(data.shape), inplace=True)[:max_options]\n",
    "    data = torch.cat([transform(x, **choice) for choice, x in zip(options, chunks(data, even_splits(len(data), len(options))))])\n",
    "    return (data[torch.argsort(i)], targets) if unshuffle else (data, targets[i])\n",
    "\n",
    "class GPUBatches():\n",
    "    def __init__(self, batch_size, transforms=(), dataset=None, shuffle=True, drop_last=False, max_options=None):\n",
    "        self.dataset, self.transforms, self.shuffle, self.max_options = dataset, transforms, shuffle, max_options\n",
    "        N = len(dataset['data'])\n",
    "        self.splits = list(range(0, N+1, batch_size))\n",
    "        if not drop_last and self.splits[-1] != N:\n",
    "            self.splits.append(N)\n",
    "     \n",
    "    def __iter__(self):\n",
    "        data, targets = self.dataset['data'], self.dataset['targets']\n",
    "        for transform in self.transforms:\n",
    "            data, targets = transformed(data, targets, transform, max_options=self.max_options, unshuffle=not self.shuffle)\n",
    "        if self.shuffle:\n",
    "            i = torch.randperm(len(data), device=device)\n",
    "            data, targets = data[i], targets[i]\n",
    "        return ({'input': x.clone(), 'target': y} for (x, y) in zip(chunks(data, self.splits), chunks(targets, self.splits)))\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.splits) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-basin",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exposed-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.graph = build_graph(net)\n",
    "        for path, (val, _) in self.graph.items(): \n",
    "            setattr(self, path.replace('/', '_'), val)\n",
    "    \n",
    "    def nodes(self):\n",
    "        return (node for node, _ in self.graph.values())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = dict(inputs)\n",
    "        for k, (node, ins) in self.graph.items():\n",
    "            #only compute nodes that are not supplied as inputs.\n",
    "            if k not in outputs: \n",
    "                outputs[k] = node(*[outputs[x] for x in ins])\n",
    "        return outputs\n",
    "    \n",
    "    def half(self):\n",
    "        for node in self.nodes():\n",
    "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
    "                node.half()\n",
    "        return self\n",
    "\n",
    "class Identity(namedtuple('Identity', [])):\n",
    "    def __call__(self, x): return x\n",
    "\n",
    "class Add(namedtuple('Add', [])):\n",
    "    def __call__(self, x, y): return x + y \n",
    "    \n",
    "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
    "    def __call__(self, x, y): return self.wx*x + self.wy*y \n",
    "\n",
    "class Mul(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    def __call__(self, x): \n",
    "        return x*self.weight\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    def forward(self, *xs): return torch.cat(xs, 1)\n",
    "\n",
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight_freeze=False, bias_freeze=False, weight_init=1.0, bias_init=0.0):\n",
    "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
    "        if weight_init is not None: self.weight.data.fill_(weight_init)\n",
    "        if bias_init is not None: self.bias.data.fill_(bias_init)\n",
    "        self.weight.requires_grad = not weight_freeze\n",
    "        self.bias.requires_grad = not bias_freeze\n",
    "        \n",
    "# Losses\n",
    "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
    "    def __call__(self, log_probs, target):\n",
    "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
    "    \n",
    "class KLLoss(namedtuple('KLLoss', [])):        \n",
    "    def __call__(self, log_probs):\n",
    "        return -log_probs.mean(dim=1)\n",
    "\n",
    "class Correct(namedtuple('Correct', [])):\n",
    "    def __call__(self, classifier, target):\n",
    "        return classifier.max(dim = 1)[1] == target\n",
    "\n",
    "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
    "    def __call__(self, x):\n",
    "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
    "\n",
    "x_ent_loss = Network({\n",
    "  'loss':  (nn.CrossEntropyLoss(reduction='none'), ['logits', 'target']),\n",
    "  'acc': (Correct(), ['logits', 'target'])\n",
    "})\n",
    "\n",
    "label_smoothing_loss = lambda alpha: Network({\n",
    "        'logprobs': (LogSoftmax(dim=1), ['logits']),\n",
    "        'KL':  (KLLoss(), ['logprobs']),\n",
    "        'xent':  (CrossEntropyLoss(), ['logprobs', 'target']),\n",
    "        'loss': (AddWeighted(wx=1-alpha, wy=alpha), ['xent', 'KL']),\n",
    "        'acc': (Correct(), ['logits', 'target']),\n",
    "    })\n",
    "\n",
    "trainable_params = lambda model: {k:p for k,p in model.named_parameters() if p.requires_grad}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-houston",
   "metadata": {},
   "source": [
    "## Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "competent-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
    "    dw.add_(weight_decay, w).mul_(-lr)\n",
    "    v.mul_(momentum).add_(dw)\n",
    "    w.add_(dw.add_(momentum, v))\n",
    "\n",
    "norm = lambda x: torch.norm(x.reshape(x.size(0),-1).float(), dim=1)[:,None,None,None]\n",
    "\n",
    "def LARS_update(w, dw, v, lr, weight_decay, momentum):\n",
    "    nesterov_update(w, dw, v, lr*(norm(w)/(norm(dw)+1e-2)).to(w.dtype), weight_decay, momentum)\n",
    "\n",
    "def zeros_like(weights):\n",
    "    return [torch.zeros_like(w) for w in weights]\n",
    "\n",
    "def optimiser(weights, param_schedule, update, state_init):\n",
    "    weights = list(weights)\n",
    "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
    "\n",
    "def opt_step(update, param_schedule, step_number, weights, opt_state):\n",
    "    step_number += 1\n",
    "    param_values = {k: f(step_number) for k, f in param_schedule.items()}\n",
    "    for w, v in zip(weights, opt_state):\n",
    "        if w.requires_grad:\n",
    "            update(w.data, w.grad.data, v, **param_values)\n",
    "    return {'update': update, 'param_schedule': param_schedule, 'step_number': step_number, 'weights': weights,  'opt_state': opt_state}\n",
    "\n",
    "LARS = partial(optimiser, update=LARS_update, state_init=zeros_like)\n",
    "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-subcommittee",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "certain-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def reduce(batches, state, steps):\n",
    "    #state: is a dictionary\n",
    "    #steps: are functions that take (batch, state)\n",
    "    #and return a dictionary of updates to the state (or None)\n",
    "    \n",
    "    for batch in chain(batches, [None]): \n",
    "    #we send an extra batch=None at the end for steps that \n",
    "    #need to do some tidying-up (e.g. log_activations)\n",
    "        for step in steps:\n",
    "            updates = step(batch, state)\n",
    "            if updates:\n",
    "                for k,v in updates.items():\n",
    "                    state[k] = v                  \n",
    "    return state\n",
    "  \n",
    "#define keys in the state dict as constants\n",
    "MODEL = 'model'\n",
    "LOSS = 'loss'\n",
    "VALID_MODEL = 'valid_model'\n",
    "OUTPUT = 'output'\n",
    "OPTS = 'optimisers'\n",
    "ACT_LOG = 'activation_log'\n",
    "WEIGHT_LOG = 'weight_log'\n",
    "\n",
    "#step definitions\n",
    "def forward(training_mode):\n",
    "    def step(batch, state):\n",
    "        if not batch: return\n",
    "        model = state[MODEL] if training_mode or (VALID_MODEL not in state) else state[VALID_MODEL]\n",
    "        if model.training != training_mode: #without the guard it's slow!\n",
    "            model.train(training_mode)\n",
    "        return {OUTPUT: state[LOSS](model(batch))}\n",
    "    return step\n",
    "\n",
    "def backward(dtype=None):\n",
    "    def step(batch, state):\n",
    "        state[MODEL].zero_grad()\n",
    "        if not batch: return\n",
    "        loss = state[OUTPUT][LOSS]\n",
    "        if dtype is not None:\n",
    "            loss = loss.to(dtype)\n",
    "        loss.sum().backward()\n",
    "    return step\n",
    "\n",
    "def opt_steps(batch, state):\n",
    "    if not batch: return\n",
    "    return {OPTS: [opt_step(**opt) for opt in state[OPTS]]}\n",
    "\n",
    "def log_activations(node_names=('loss', 'acc')):\n",
    "    def step(batch, state):\n",
    "        if '_tmp_logs_' not in state: \n",
    "            state['_tmp_logs_'] = []\n",
    "        if batch:\n",
    "            state['_tmp_logs_'].extend((k, state[OUTPUT][k].detach()) for k in node_names)\n",
    "        else:\n",
    "            res = {k: to_numpy(torch.cat(xs)).astype(float) for k, xs in group_by_key(state['_tmp_logs_']).items()}\n",
    "            del state['_tmp_logs_']\n",
    "            return {ACT_LOG: res}\n",
    "    return step\n",
    "\n",
    "epoch_stats = lambda state: {k: np.mean(v) for k, v in state[ACT_LOG].items()}\n",
    "\n",
    "default_train_steps = (forward(training_mode=True), log_activations(('loss', 'acc')), backward(), opt_steps)\n",
    "default_valid_steps = (forward(training_mode=False), log_activations(('loss', 'acc')))\n",
    "\n",
    "\n",
    "def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n",
    "                on_epoch_end=(lambda state: state)):\n",
    "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
    "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(include_in_total=False) #DAWNBench rules\n",
    "    return {\n",
    "        'train': union({'time': train_time}, train_summary), \n",
    "        'valid': union({'time': valid_time}, valid_summary), \n",
    "        'total time': timer.total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-nitrogen",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "official-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(c_in, c_out):\n",
    "    return {\n",
    "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
    "        'bn': BatchNorm(c_out), \n",
    "        'relu': nn.ReLU(True)\n",
    "    }\n",
    "\n",
    "def residual(c):\n",
    "    return {\n",
    "        'in': Identity(),\n",
    "        'res1': conv_bn(c, c),\n",
    "        'res2': conv_bn(c, c),\n",
    "        'add': (Add(), ['in', 'res2/relu']),\n",
    "    }\n",
    "\n",
    "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3')):\n",
    "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
    "    n = {\n",
    "        'input': (None, []),\n",
    "        'prep': conv_bn(3, channels['prep']),\n",
    "        'layer1': dict(conv_bn(channels['prep'], channels['layer1']), pool=pool),\n",
    "        'layer2': dict(conv_bn(channels['layer1'], channels['layer2']), pool=pool),\n",
    "        'layer3': dict(conv_bn(channels['layer2'], channels['layer3']), pool=pool),\n",
    "        'pool': nn.MaxPool2d(4),\n",
    "        'flatten': Flatten(),\n",
    "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
    "        'logits': Mul(weight),\n",
    "    }\n",
    "    for layer in res_layers:\n",
    "        n[layer]['residual'] = residual(channels[layer])\n",
    "    for layer in extra_layers:\n",
    "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-fleet",
   "metadata": {},
   "source": [
    "## Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "smart-crest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "dataset = cifar10(root=DATA_DIR)\n",
    "timer = Timer()\n",
    "\n",
    "transforms = [\n",
    "    partial(normalise, mean=np.array(cifar10_mean, dtype=np.float32), std=np.array(cifar10_std, dtype=np.float32)),\n",
    "    partial(transpose, source='NHWC', target='NCHW'), \n",
    "]\n",
    "\n",
    "train_set = list(zip(*preprocess(dataset['train'], [partial(pad, border=4)] + transforms).values()))\n",
    "valid_set = list(zip(*preprocess(dataset['valid'], transforms).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-barbados",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "commercial-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run 0 at 2022-05-09 16:08:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userfs/s/sb2444/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1      11.1700       1.6505       0.4013       0.9720       1.2502       0.5523      11.1700\n",
      "           2       9.6992       0.9495       0.6633       0.6742       1.0622       0.6455       9.6992\n",
      "           3       9.7166       0.7371       0.7415       0.6752       0.7065       0.7585       9.7166\n",
      "           4       9.7440       0.6336       0.7780       0.7533       0.7697       0.7316       9.7440\n",
      "           5       9.7515       0.5612       0.8044       0.6779       0.6404       0.7782       9.7515\n",
      "           6       9.7514       0.4948       0.8291       0.6827       0.4660       0.8386       9.7514\n",
      "           7       9.7714       0.4439       0.8484       0.6839       0.7421       0.7393       9.7714\n",
      "           8       9.7786       0.4126       0.8583       0.6822       0.5839       0.8036       9.7786\n",
      "           9       9.8039       0.3855       0.8682       0.6839       0.4748       0.8379       9.8039\n",
      "          10       9.8030       0.3636       0.8753       0.6986       0.7079       0.7603       9.8030\n",
      "          11       9.7976       0.3458       0.8828       0.6850       0.4313       0.8519       9.7976\n",
      "          12       9.7975       0.3225       0.8909       0.6869       0.4553       0.8516       9.7975\n",
      "          13       9.8078       0.3084       0.8942       0.6854       0.5242       0.8245       9.8078\n",
      "          14       9.8286       0.2864       0.9031       0.6845       0.3618       0.8775       9.8286\n",
      "          15       9.7990       0.2725       0.9083       0.6846       0.3959       0.8672       9.7990\n",
      "          16       9.8314       0.2493       0.9172       0.6895       0.3621       0.8723       9.8314\n",
      "          17       9.8261       0.2292       0.9229       0.6866       0.2878       0.9021       9.8261\n",
      "          18       9.8129       0.2107       0.9289       0.6877       0.3878       0.8735       9.8129\n",
      "          19       9.8139       0.1898       0.9366       0.6871       0.3114       0.8938       9.8139\n",
      "          20       9.8343       0.1666       0.9443       0.6956       0.2942       0.9031       9.8343\n",
      "          21       9.8528       0.1382       0.9555       0.6886       0.2539       0.9170       9.8528\n",
      "          22       9.8345       0.1185       0.9618       0.6877       0.2201       0.9263       9.8345\n",
      "          23       9.8437       0.0930       0.9714       0.6874       0.1902       0.9368       9.8437\n",
      "          24       9.8066       0.0763       0.9778       0.6867       0.1843       0.9409       9.8066\n",
      "Starting Run 1 at 2022-05-09 16:13:10\n",
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       9.8388       1.6389       0.4090       0.6834       1.6404       0.4633       9.8388\n",
      "           2       9.8480       0.9527       0.6607       0.7187       1.0685       0.6443       9.8480\n",
      "           3       9.8412       0.7430       0.7407       0.6944       0.9883       0.6769       9.8412\n",
      "           4       9.8268       0.6263       0.7829       0.6817       0.8453       0.7162       9.8268\n",
      "           5       9.8515       0.5650       0.8023       0.6823       0.6353       0.7840       9.8515\n",
      "           6       9.8413       0.4975       0.8296       0.6833       0.6346       0.7827       9.8413\n",
      "           7       9.8574       0.4492       0.8451       0.6828       0.5272       0.8179       9.8574\n",
      "           8       9.8500       0.4174       0.8557       0.6817       0.5448       0.8119       9.8500\n",
      "           9       9.8592       0.3804       0.8711       0.6862       0.5470       0.8119       9.8592\n",
      "          10       9.8376       0.3671       0.8742       0.6818       0.4216       0.8566       9.8376\n",
      "          11       9.8276       0.3465       0.8813       0.7301       0.6057       0.8043       9.8276\n",
      "          12       9.8448       0.3291       0.8882       0.6801       0.3844       0.8716       9.8448\n",
      "          13       9.8474       0.3061       0.8964       0.6828       0.4278       0.8587       9.8474\n",
      "          14       9.8490       0.2905       0.9014       0.6830       0.3974       0.8668       9.8490\n",
      "          15       9.8439       0.2723       0.9084       0.6807       0.3559       0.8800       9.8439\n",
      "          16       9.8449       0.2491       0.9164       0.6880       0.4868       0.8437       9.8449\n",
      "          17       9.8339       0.2341       0.9217       0.6823       0.3062       0.8970       9.8339\n",
      "          18       9.8324       0.2116       0.9304       0.6820       0.3042       0.8948       9.8324\n",
      "          19       9.8441       0.1850       0.9386       0.6834       0.3761       0.8814       9.8441\n",
      "          20       9.8576       0.1682       0.9451       0.6809       0.3221       0.8962       9.8576\n",
      "          21       9.8307       0.1406       0.9542       0.6811       0.2820       0.9076       9.8307\n",
      "          22       9.8427       0.1167       0.9637       0.6936       0.2220       0.9262       9.8427\n",
      "          23       9.8372       0.0959       0.9712       0.6945       0.1989       0.9338       9.8372\n",
      "          24       9.8709       0.0751       0.9788       0.6829       0.1841       0.9381       9.8709\n",
      "Starting Run 2 at 2022-05-09 16:17:23\n",
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       9.8665       1.6435       0.4107       0.6995       1.2910       0.5369       9.8665\n",
      "           2       9.8679       0.9431       0.6626       0.6973       1.3019       0.5876       9.8679\n",
      "           3       9.8563       0.7374       0.7412       0.6916       0.7385       0.7410       9.8563\n",
      "           4       9.8522       0.6279       0.7807       0.6921       0.7517       0.7356       9.8522\n",
      "           5       9.8477       0.5610       0.8056       0.6877       0.6821       0.7639       9.8477\n",
      "           6       9.8521       0.5033       0.8266       0.6986       0.5922       0.8015       9.8521\n",
      "           7       9.8304       0.4416       0.8483       0.6873       0.5829       0.7983       9.8304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3c12a599663a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_ent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch=={epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5dcfbef4d141>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(state, timer, train_batches, valid_batches, train_steps, valid_steps, on_epoch_end)\u001b[0m\n\u001b[1;32m     69\u001b[0m def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n\u001b[1;32m     70\u001b[0m                 on_epoch_end=(lambda state: state)):\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtrain_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mvalid_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_in_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#DAWNBench rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     return {\n",
      "\u001b[0;32m<ipython-input-13-5dcfbef4d141>\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(batches, state, steps)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#and return a dictionary of updates to the state (or None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#we send an extra batch=None at the end for steps that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#need to do some tidying-up (e.g. log_activations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4453d10ff87a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_choices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_choices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=24\n",
    "lr_schedule = PiecewiseLinear([0, 5, epochs], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "train_transforms = [Crop(32, 32), FlipLR(), Cutout(8, 8)]\n",
    "N_runs = 3\n",
    "\n",
    "train_batches = DataLoader(Transform(train_set, train_transforms), batch_size, shuffle=True, set_random_choices=True, drop_last=True)\n",
    "valid_batches = DataLoader(valid_set, batch_size, shuffle=False, drop_last=False)\n",
    "lr = lambda step: lr_schedule(step/len(train_batches))/batch_size\n",
    "\n",
    "summaries = []\n",
    "for i in range(N_runs):\n",
    "    print(f'Starting Run {i} at {localtime()}')\n",
    "    model = Network(net()).to(device).half()\n",
    "    opts = [SGD(trainable_params(model).values(), {'lr': lr, 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)})]\n",
    "    logs, state = Table(), {MODEL: model, LOSS: x_ent_loss, OPTS: opts}\n",
    "    for epoch in range(epochs):\n",
    "        logs.append(union({'epoch': epoch+1}, train_epoch(state, Timer(torch.cuda.synchronize), train_batches, valid_batches)))\n",
    "logs.df().query(f'epoch=={epochs}')[['train_acc', 'valid_acc']].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
